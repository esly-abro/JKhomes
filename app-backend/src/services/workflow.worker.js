/**
 * Workflow Worker
 * 
 * BullMQ workers that process workflow execution jobs.
 * This replaces the old setInterval + MongoDB polling approach.
 *
 * Workers:
 *   1. Trigger Worker   â€” Receives events, finds matching workflows, creates execution instances
 *   2. Execute Worker   â€” Processes individual node execution steps
 *   3. Timeout Worker   â€” Handles waiting run timeouts
 *
 * Design principles:
 *   - Each lead gets its own execution instance (1000 leads â†’ 1000 executions)
 *   - Delay nodes use BullMQ delayed jobs (survives restart, no setTimeout)
 *   - All execution state stored in MongoDB (AutomationRun)
 *   - Workers are stateless â€” can scale horizontally
 *   - Failed jobs retry with exponential backoff (3 attempts)
 *   - Dead letter queue for unrecoverable failures
 */

const { Worker } = require('bullmq');
const { getRedisConfig } = require('../config/redis');
const { QUEUE_NAMES, enqueueNodeExecution, enqueueTimeoutCheck, getDeadLetterQueue } = require('./workflow.queue');
const logger = require('../utils/logger');

// MongoDB models
const Automation = require('../models/Automation');
const AutomationRun = require('../models/AutomationRun');
const AutomationJob = require('../models/AutomationJob');
const Lead = require('../models/Lead');
const ExecutionLog = require('../models/ExecutionLog');

// Existing modular engine components (reuse all of them)
const triggers = require('./workflow.triggers');
const conditions = require('./workflow.conditions');
const executors = require('./workflow.executors');
const resume = require('./workflow.resume');
const recovery = require('./workflow.recovery');

// â”€â”€â”€ Worker Instance ID (unique per process for horizontal scaling) â”€â”€

const WORKER_ID = `w-${process.pid}-${Date.now().toString(36)}`;

// â”€â”€â”€ Concurrency Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const TRIGGER_CONCURRENCY = parseInt(process.env.WORKFLOW_TRIGGER_CONCURRENCY, 10) || 5;
const EXECUTE_CONCURRENCY = parseInt(process.env.WORKFLOW_EXECUTE_CONCURRENCY, 10) || 10;
const TIMEOUT_CONCURRENCY = parseInt(process.env.WORKFLOW_TIMEOUT_CONCURRENCY, 10) || 3;

// â”€â”€â”€ Execution Timeout (wall-clock cap for any node execution) â”€â”€

const NODE_EXECUTION_TIMEOUT_MS = parseInt(process.env.NODE_EXECUTION_TIMEOUT_MS, 10) || 120000; // 2 min default

// â”€â”€â”€ Worker Instances â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

let _triggerWorker = null;
let _executeWorker = null;
let _timeoutWorker = null;

// â”€â”€â”€ Helper: Execution Timeout Wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Wraps a promise with a wall-clock timeout.
 * If the promise doesn't resolve/reject within `ms`, rejects with a timeout error.
 * This prevents hung executors (API calls that never respond) from blocking workers.
 */
function withTimeout(promise, ms, nodeLabel) {
    return new Promise((resolve, reject) => {
        const timer = setTimeout(() => {
            reject(new Error(`Node "${nodeLabel}" timed out after ${ms}ms`));
        }, ms);
        promise.then(
            (val) => { clearTimeout(timer); resolve(val); },
            (err) => { clearTimeout(timer); reject(err); }
        );
    });
}

// â”€â”€â”€ Helper: Structured Execution Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Write a structured execution log entry.
 * Each node transition (pending â†’ running â†’ success/failed) gets logged.
 * 
 * Format: { nodeId, status, timestamp, message, ...metadata }
 */
async function logExecution({ run, automation, lead, nodeId, nodeData, status, message, error, attempt, duration, metadata }) {
    try {
        await ExecutionLog.create({
            organizationId: run.organizationId,
            automationRun: run._id,
            automation: automation?._id || automation,
            lead: lead?._id || lead,
            nodeId,
            nodeType: nodeData?.type,
            nodeLabel: nodeData?.label || nodeId,
            status,
            message,
            error,
            duration,
            attempt,
            workerId: WORKER_ID,
            metadata,
            timestamp: new Date(),
        });
    } catch (e) {
        // Never let logging failure crash execution
        logger.error(`Failed to write execution log: ${e.message}`);
    }
}

// â”€â”€â”€ Helper: Schedule Next Nodes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Given a completed node, find its downstream nodes and enqueue them.
 * This is the BullMQ replacement for the old engine's scheduleNextNodes.
 */
async function scheduleNextNodes(run, automation, lead, currentNodeId, handle = null, extraDelay = 0) {
    let nextNodes = automation.getNextNodes(currentNodeId, handle);

    // Fallback: if handle-specific lookup returned nothing, try without handle
    if (nextNodes.length === 0 && handle) {
        nextNodes = automation.getNextNodes(currentNodeId);
    }

    if (nextNodes.length === 0) {
        // Check if there are any pending BullMQ jobs for this run
        // If not, the run is complete
        const pendingJobs = await AutomationJob.countDocuments({
            automationRun: run._id,
            status: 'pending',
        });

        if (pendingJobs === 0) {
            run.status = 'completed';
            run.completedAt = new Date();
            await run.save();

            automation.successCount = (automation.successCount || 0) + 1;
            await automation.save();

            logger.info(`âœ… Workflow run completed: ${automation.name} (run ${run._id})`);
        }
        return 0;
    }

    for (const { node, edge } of nextNodes) {
        let delay = extraDelay;

        // If next node IS a delay/wait, calculate its delay and pass it
        if (node.data?.type === 'delay' || node.data?.type === 'wait') {
            delay = conditions.calculateDelay(node.data.config);
        }

        // Record in execution path
        run.executionPath.push({
            nodeId: node.id,
            nodeType: node.data?.type || node.type,
            nodeLabel: node.data?.label || node.id,
            label: node.data?.label || node.id,
            status: 'pending',
            scheduledFor: new Date(Date.now() + delay),
        });
        await run.save();

        // Also record in AutomationJob for backwards compatibility / monitoring
        const jobRecord = new AutomationJob({
            automation: automation._id,
            automationRun: run._id,
            lead: lead._id,
            organizationId: lead.organizationId,
            nodeId: node.id,
            nodeType: node.data?.type || node.type,
            nodeData: node.data,
            edgeId: edge?.id,
            status: 'pending',
            scheduledFor: new Date(Date.now() + delay),
            attempts: 0,
            maxAttempts: node.data?.config?.maxRetries || 3,
        });
        await jobRecord.save();

        // Enqueue in BullMQ
        await enqueueNodeExecution({
            runId: run._id,
            automationId: automation._id,
            leadId: lead._id,
            nodeId: node.id,
            nodeData: node.data,
            edgeId: edge?.id,
            organizationId: lead.organizationId,
            delay,
        });
    }

    return nextNodes.length;
}

// â”€â”€â”€ Helper: Update Execution Path Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function updateExecutionPathStatus(run, nodeId, status, result = null) {
    const pathIndex = run.executionPath.findIndex(
        (p) => p.nodeId === nodeId && (p.status === 'pending' || p.status === 'running')
    );
    if (pathIndex >= 0) {
        run.executionPath[pathIndex].status = status;
        if (status === 'completed' || status === 'failed') {
            run.executionPath[pathIndex].completedAt = new Date();
        }
        if (result) {
            run.executionPath[pathIndex].result = result;
        }
    }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TRIGGER WORKER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Process a trigger event:
 *   1. Load the lead
 *   2. Find all active automations matching the trigger + org
 *   3. For each, create an AutomationRun and enqueue the first node(s)
 */
async function processTriggerJob(job) {
    const { event, leadId, organizationId, changes, context, forceAutomationId } = job.data;

    logger.info(`ğŸ”” Processing trigger: ${event} for lead ${leadId}`);

    const lead = await Lead.findById(leadId);
    if (!lead) {
        logger.warn(`Lead ${leadId} not found, skipping trigger`);
        return { skipped: true, reason: 'lead_not_found' };
    }

    // Map event names to triggerType enum values
    const eventToTriggerType = {
        'lead.created': 'newLead',
        'lead.updated': 'leadUpdated',
        'appointment.scheduled': 'siteVisitScheduled',
        'siteVisit.scheduled': 'siteVisitScheduled',
    };

    const triggerType = eventToTriggerType[event] || event;

    // Find all active automations matching this trigger and organization
    let automations;
    if (forceAutomationId) {
        // Manual trigger â€” only run the specified automation
        const forced = await Automation.findById(forceAutomationId);
        automations = forced ? [forced] : [];
    } else {
        const query = {
            isActive: true,
            triggerType,
        };
        if (organizationId) {
            query.organizationId = organizationId;
        }
        automations = await Automation.find(query);
    }

    if (automations.length === 0) {
        logger.info(`No active automations for trigger ${triggerType} (org: ${organizationId})`);
        return { matched: 0 };
    }

    logger.info(`Found ${automations.length} automation(s) for trigger ${triggerType}`);

    const results = [];

    for (const automation of automations) {
        try {
            // â”€â”€ Trigger condition filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (automation.triggerConditions) {
                const passes = triggers.matchesTriggerConditions
                    ? triggers.matchesTriggerConditions(lead, automation.triggerConditions)
                    : true;
                if (!passes) {
                    logger.info(`Lead ${lead.name} does not match trigger conditions for ${automation.name}`);
                    results.push({ automationId: automation._id, skipped: true, reason: 'conditions_not_met' });
                    continue;
                }
            }

            // â”€â”€ Duplicate prevention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (automation.preventDuplicates || automation.runOncePerLead) {
                const existingRun = await AutomationRun.findOne({
                    automation: automation._id,
                    lead: lead._id,
                    status: { $in: ['running', 'waiting_for_response', 'pending'] },
                });
                if (existingRun) {
                    logger.info(`Skipping duplicate run for lead ${lead.name} on ${automation.name}`);
                    results.push({ automationId: automation._id, skipped: true, reason: 'duplicate' });
                    continue;
                }
            }

            // â”€â”€ Cooldown check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (automation.cooldownPeriod) {
                const cooldownMs = automation.cooldownPeriod * 60 * 1000;
                const recentRun = await AutomationRun.findOne({
                    automation: automation._id,
                    lead: lead._id,
                    startedAt: { $gt: new Date(Date.now() - cooldownMs) },
                });
                if (recentRun) {
                    logger.info(`Lead ${lead.name} in cooldown for ${automation.name}`);
                    results.push({ automationId: automation._id, skipped: true, reason: 'cooldown' });
                    continue;
                }
            }

            // â”€â”€ Create execution instance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            const run = new AutomationRun({
                organizationId: lead.organizationId || organizationId,
                automation: automation._id,
                lead: lead._id,
                status: 'running',
                startedAt: new Date(),
                triggeredBy: context?.manualTrigger ? 'manual' : 'system',
                context: {
                    lead: {
                        _id: lead._id,
                        name: lead.name,
                        phone: lead.phone,
                        email: lead.email,
                        status: lead.status,
                        source: lead.source,
                        budget: lead.budget,
                        location: lead.location,
                        assignedTo: lead.assignedTo,
                    },
                    previousResults: {},
                    triggerEvent: event,
                    ...(changes ? { changes } : {}),
                },
                executionPath: [],
            });
            await run.save();

            // Update automation stats
            automation.runsCount = (automation.runsCount || 0) + 1;
            automation.lastRunAt = new Date();
            await automation.save();

            // â”€â”€ Find start nodes & enqueue first steps â”€â”€
            const startNodes =
                automation.findNodesByType('trigger').length > 0
                    ? automation.findNodesByType('trigger')
                    : automation.findNodesByType('start');

            if (startNodes.length === 0) {
                logger.error(`No trigger/start node in automation ${automation.name}`);
                run.status = 'failed';
                run.error = 'No trigger or start node found';
                await run.save();
                results.push({ automationId: automation._id, error: 'no_start_node' });
                continue;
            }

            let nodesEnqueued = 0;
            for (const startNode of startNodes) {
                const nextNodes = automation.getNextNodes(startNode.id);
                for (const { node, edge } of nextNodes) {
                    let delay = 0;
                    if (node.data?.type === 'delay' || node.data?.type === 'wait') {
                        delay = conditions.calculateDelay(node.data.config);
                    }

                    // Record in execution path
                    run.executionPath.push({
                        nodeId: node.id,
                        nodeType: node.data?.type || node.type,
                        nodeLabel: node.data?.label || node.id,
                        label: node.data?.label || node.id,
                        status: 'pending',
                        scheduledFor: new Date(Date.now() + delay),
                    });

                    // Record in AutomationJob for monitoring
                    const jobRecord = new AutomationJob({
                        automation: automation._id,
                        automationRun: run._id,
                        lead: lead._id,
                        organizationId: lead.organizationId,
                        nodeId: node.id,
                        nodeType: node.data?.type || node.type,
                        nodeData: node.data,
                        edgeId: edge?.id,
                        status: 'pending',
                        scheduledFor: new Date(Date.now() + delay),
                        attempts: 0,
                        maxAttempts: 3,
                    });
                    await jobRecord.save();

                    // Enqueue in BullMQ
                    await enqueueNodeExecution({
                        runId: run._id,
                        automationId: automation._id,
                        leadId: lead._id,
                        nodeId: node.id,
                        nodeData: node.data,
                        edgeId: edge?.id,
                        organizationId: lead.organizationId,
                        delay,
                    });

                    nodesEnqueued++;
                }
            }

            await run.save();

            logger.info(`â–¶ï¸ Started automation "${automation.name}" for lead "${lead.name}" â€” ${nodesEnqueued} node(s) enqueued (run ${run._id})`);
            results.push({ automationId: automation._id, runId: run._id, nodesEnqueued });

        } catch (err) {
            logger.error(`Error starting automation ${automation.name}: ${err.message}`);
            results.push({ automationId: automation._id, error: err.message });
        }
    }

    return { matched: automations.length, results };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// EXECUTE WORKER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Process a single node execution step:
 *   1. Load run, automation, lead
 *   2. Verify run is still active
 *   3. Execute the node via existing executors module
 *   4. Handle result (success â†’ next nodes, waiting â†’ pause, condition â†’ branch, delay â†’ delayed job)
 *   5. On failure â†’ retry or dead-letter
 */
async function processExecuteJob(job) {
    const { runId, automationId, leadId, nodeId, nodeData, edgeId } = job.data;
    const nodeLabel = nodeData?.label || nodeData?.type || nodeId;

    logger.info(`âš™ï¸  Executing node: "${nodeLabel}" (${nodeData?.type}) [run ${runId}]`);

    // Load entities
    const [run, automation, lead] = await Promise.all([
        AutomationRun.findById(runId),
        Automation.findById(automationId),
        Lead.findById(leadId),
    ]);

    // â”€â”€ Guard checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (!run) {
        logger.warn(`Run ${runId} not found, discarding job`);
        return { discarded: true, reason: 'run_not_found' };
    }
    if (run.status === 'cancelled' || run.status === 'completed') {
        logger.info(`Run ${runId} already ${run.status}, skipping node "${nodeLabel}"`);
        return { discarded: true, reason: `run_${run.status}` };
    }
    if (!automation) {
        logger.error(`Automation ${automationId} not found`);
        run.status = 'failed';
        run.error = 'Automation not found';
        await run.save();
        return { failed: true, reason: 'automation_not_found' };
    }
    if (!lead) {
        logger.error(`Lead ${leadId} not found`);
        run.status = 'failed';
        run.error = 'Lead not found';
        await run.save();
        return { failed: true, reason: 'lead_not_found' };
    }

    // Update execution path â†’ running
    updateExecutionPathStatus(run, nodeId, 'running');
    run.currentNodeId = nodeId;
    await run.save();

    // Update corresponding AutomationJob record
    await AutomationJob.findOneAndUpdate(
        { automationRun: run._id, nodeId, status: 'pending' },
        { status: 'processing', lastAttemptAt: new Date(), attempts: job.attemptsMade + 1 }
    );

    // Log: running
    await logExecution({
        run, automation, lead, nodeId, nodeData,
        status: 'running',
        message: `Executing node "${nodeLabel}" (${nodeData?.type})`,
        attempt: job.attemptsMade + 1,
    });

    // â”€â”€ Build a minimal job-like object for the executors â”€â”€â”€â”€â”€â”€â”€â”€
    // The existing executors expect a job object with these fields
    const pseudoJob = {
        _id: job.id,
        automation: automation._id,
        automationRun: run._id,
        lead: lead._id,
        nodeId,
        nodeType: nodeData?.type || 'unknown',
        nodeData,
        edgeId,
        attempts: job.attemptsMade + 1,
        maxAttempts: 3,
    };

    // â”€â”€ Execute the node (with wall-clock timeout) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const startTime = Date.now();
    let result;
    try {
        result = await withTimeout(
            executors.executeNode(nodeData, lead, run, automation, pseudoJob),
            NODE_EXECUTION_TIMEOUT_MS,
            nodeLabel
        );
    } catch (timeoutOrExecErr) {
        const elapsed = Date.now() - startTime;
        const isTimeout = timeoutOrExecErr.message?.includes('timed out after');

        await logExecution({
            run, automation, lead, nodeId, nodeData,
            status: isTimeout ? 'timeout' : 'failed',
            message: timeoutOrExecErr.message,
            error: timeoutOrExecErr.message,
            duration: elapsed,
            attempt: job.attemptsMade + 1,
        });

        // Update AutomationJob
        await AutomationJob.findOneAndUpdate(
            { automationRun: run._id, nodeId, status: 'processing' },
            { status: 'failed', lastError: timeoutOrExecErr.message, completedAt: new Date() }
        );

        // Throw to trigger BullMQ retry
        throw timeoutOrExecErr;
    }

    const elapsed = Date.now() - startTime;

    // â”€â”€ Handle waiting nodes (WhatsApp response, AI call result) â”€
    if (result.waiting) {
        logger.info(`â¸ï¸  Node "${nodeLabel}" is waiting (${result.waitingFor || 'response'}) [run ${runId}]`);

        updateExecutionPathStatus(run, nodeId, 'waiting', result);
        await run.save();

        await logExecution({
            run, automation, lead, nodeId, nodeData,
            status: 'waiting',
            message: `Waiting for ${result.waitingFor || 'response'}`,
            duration: elapsed,
            attempt: job.attemptsMade + 1,
            metadata: { waitingFor: result.waitingFor, timeoutAt: result.timeoutAt },
        });

        // Update AutomationJob
        await AutomationJob.findOneAndUpdate(
            { automationRun: run._id, nodeId, status: 'processing' },
            { status: 'waiting', result }
        );

        // Schedule timeout check if there's a timeout configured
        if (result.timeoutAt) {
            const timeoutDelay = new Date(result.timeoutAt).getTime() - Date.now();
            if (timeoutDelay > 0) {
                const timeoutType = result.waitingFor === 'call' ? 'call' : 'response';
                await enqueueTimeoutCheck(run._id, timeoutType, timeoutDelay);
            }
        }

        return { waiting: true, nodeId };
    }

    // â”€â”€ Handle condition nodes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (executors.isConditionNodeType(nodeData?.type)) {
        const handle = result.passed ? 'true' : 'false';

        logger.info(`ğŸ”€ Condition "${nodeLabel}" evaluated â†’ ${handle} [run ${runId}]`);

        updateExecutionPathStatus(run, nodeId, 'completed', result);
        await run.save();

        await logExecution({
            run, automation, lead, nodeId, nodeData,
            status: 'success',
            message: `Condition evaluated â†’ ${handle}`,
            duration: elapsed,
            attempt: job.attemptsMade + 1,
            metadata: { conditionResult: handle },
        });

        // Update AutomationJob
        await AutomationJob.findOneAndUpdate(
            { automationRun: run._id, nodeId, status: 'processing' },
            { status: 'completed', result, completedAt: new Date() }
        );

        // Branch: enqueue the correct downstream path
        await scheduleNextNodes(run, automation, lead, nodeId, handle);
        return { condition: true, handle, nodeId };
    }

    // â”€â”€ Handle delay nodes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (result.delayed) {
        const delayMs = conditions.calculateDelay(nodeData?.config);

        logger.info(`â±ï¸  Delay node "${nodeLabel}": ${delayMs}ms [run ${runId}]`);

        updateExecutionPathStatus(run, nodeId, 'completed', result);
        await run.save();

        await logExecution({
            run, automation, lead, nodeId, nodeData,
            status: 'success',
            message: `Delay scheduled: ${delayMs}ms`,
            duration: elapsed,
            attempt: job.attemptsMade + 1,
            metadata: { delayMs },
        });

        // Update AutomationJob
        await AutomationJob.findOneAndUpdate(
            { automationRun: run._id, nodeId, status: 'processing' },
            { status: 'completed', result, completedAt: new Date() }
        );

        // Schedule next nodes with the delay
        await scheduleNextNodes(run, automation, lead, nodeId, null, delayMs);
        return { delayed: true, delayMs, nodeId };
    }

    // â”€â”€ Handle success â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (result.success || result.skipped) {
        logger.info(`âœ… Node "${nodeLabel}" completed [run ${runId}]`);

        // Store result in execution context for downstream nodes
        if (result.data || result.messageId || result.callId) {
            run.context = run.context || {};
            run.context.previousResults = run.context.previousResults || {};
            run.context.previousResults[nodeId] = {
                type: nodeData?.type,
                label: nodeLabel,
                ...(result.data || {}),
                messageId: result.messageId,
                callId: result.callId,
                completedAt: new Date().toISOString(),
            };
        }

        updateExecutionPathStatus(run, nodeId, 'completed', result);
        await run.save();

        await logExecution({
            run, automation, lead, nodeId, nodeData,
            status: 'success',
            message: result.skipped ? `Node skipped` : `Node completed successfully`,
            duration: elapsed,
            attempt: job.attemptsMade + 1,
        });

        // Update AutomationJob
        await AutomationJob.findOneAndUpdate(
            { automationRun: run._id, nodeId, status: 'processing' },
            { status: 'completed', result, completedAt: new Date() }
        );

        // Enqueue next nodes
        await scheduleNextNodes(run, automation, lead, nodeId);
        return { success: true, nodeId };
    }

    // â”€â”€ Handle explicit failure from executor â”€â”€â”€
    const errMsg = result.error || 'Unknown executor error';
    logger.error(`âŒ Node "${nodeLabel}" failed: ${errMsg} [run ${runId}]`);

    await logExecution({
        run, automation, lead, nodeId, nodeData,
        status: 'failed',
        message: errMsg,
        error: errMsg,
        duration: elapsed,
        attempt: job.attemptsMade + 1,
    });

    // Update AutomationJob
    await AutomationJob.findOneAndUpdate(
        { automationRun: run._id, nodeId, status: 'processing' },
        { status: 'failed', lastError: errMsg, completedAt: new Date() }
    );

    // Throw to trigger BullMQ retry
    throw new Error(errMsg);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TIMEOUT WORKER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function processTimeoutJob(job) {
    const { runId, type } = job.data;

    logger.info(`â° Processing ${type} timeout for run ${runId}`);

    const run = await AutomationRun.findById(runId).populate('lead').populate('automation');
    if (!run) {
        logger.warn(`Run ${runId} not found for timeout processing`);
        return { discarded: true };
    }

    // Only process if still waiting
    if (run.status !== 'waiting_for_response') {
        logger.info(`Run ${runId} is no longer waiting (status: ${run.status}), skipping timeout`);
        return { skipped: true, reason: `not_waiting` };
    }

    // Find the waiting node for logging
    const waitingEntry = (run.executionPath || []).find(e => e.status === 'waiting');
    const waitingNodeId = waitingEntry?.nodeId || 'unknown';
    const waitingNodeData = { type: waitingEntry?.nodeType, label: waitingEntry?.nodeLabel };

    await logExecution({
        run,
        automation: run.automation,
        lead: run.lead,
        nodeId: waitingNodeId,
        nodeData: waitingNodeData,
        status: 'timeout',
        message: `${type} timeout triggered â€” resuming workflow`,
        attempt: job.attemptsMade + 1,
        metadata: { timeoutType: type },
    });

    // Use the BullMQ-compatible scheduleNode function
    const scheduleNodeFn = async (run, automation, lead, node, edge, delay = 0) => {
        run.executionPath.push({
            nodeId: node.id,
            nodeType: node.data?.type || node.type,
            nodeLabel: node.data?.label || node.id,
            label: node.data?.label || node.id,
            status: 'pending',
            scheduledFor: new Date(Date.now() + delay),
        });
        await run.save();

        await enqueueNodeExecution({
            runId: run._id,
            automationId: run.automation._id || run.automation,
            leadId: run.lead._id || run.lead,
            nodeId: node.id,
            nodeData: node.data,
            edgeId: edge?.id,
            organizationId: run.organizationId,
            delay,
        });
    };

    if (type === 'response') {
        await resume.resumeFromTimeout(run, scheduleNodeFn);
    } else if (type === 'call') {
        await resume.resumeFromCallTimeout(run, scheduleNodeFn);
    }

    return { processed: true, type };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FAILED JOB HANDLER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function handleFailedExecuteJob(job, err) {
    const { runId, automationId, leadId, nodeId, nodeData } = job.data || {};
    const nodeLabel = nodeData?.label || nodeData?.type || nodeId;

    // If this was the final attempt (all retries exhausted), mark run as failed + DLQ
    if (job.attemptsMade >= (job.opts?.attempts || 3)) {
        logger.error(`ğŸ’€ Node "${nodeLabel}" exhausted all retries [run ${runId}]: ${err.message}`);

        try {
            const run = await AutomationRun.findById(runId);
            if (run && run.status === 'running') {
                // Try failure path first
                const failResult = await recovery.takeFailurePath(
                    { nodeId, nodeData, automationRun: run._id, automation: run.automation },
                    err,
                    async (r, auto, lead, node, edge, delay) => {
                        await enqueueNodeExecution({
                            runId: r._id,
                            automationId: auto._id,
                            leadId: lead._id,
                            nodeId: node.id,
                            nodeData: node.data,
                            edgeId: edge?.id,
                            organizationId: r.organizationId,
                            delay,
                        });
                    }
                );

                if (!failResult || !failResult.success) {
                    run.status = 'failed';
                    run.error = `Node "${nodeLabel}" failed after ${job.attemptsMade} attempts: ${err.message}`;
                    updateExecutionPathStatus(run, nodeId, 'failed', { error: err.message });
                    await run.save();

                    // Notify admin
                    await recovery.notifyAdminOfFailure(
                        { nodeId, nodeData, automationRun: run._id },
                        err
                    ).catch(() => {});
                }
            }

            // Update AutomationJob record
            await AutomationJob.findOneAndUpdate(
                { automationRun: runId, nodeId, status: { $in: ['pending', 'processing'] } },
                { status: 'failed', lastError: err.message, completedAt: new Date() }
            );

            // â”€â”€ Dead Letter Queue: persist for manual inspection/replay â”€â”€
            try {
                const dlq = getDeadLetterQueue();
                await dlq.add('dead-letter', {
                    originalJobId: job.id,
                    originalJobName: job.name,
                    runId,
                    automationId,
                    leadId,
                    nodeId,
                    nodeData,
                    error: err.message,
                    stack: err.stack,
                    attemptsMade: job.attemptsMade,
                    failedAt: new Date().toISOString(),
                    workerId: WORKER_ID,
                }, {
                    jobId: `dlq:${runId}:${nodeId}:${Date.now()}`,
                });
                logger.warn(`ğŸ“¦ Job moved to DLQ: "${nodeLabel}" [run ${runId}]`);
            } catch (dlqErr) {
                logger.error(`Failed to enqueue DLQ job: ${dlqErr.message}`);
            }

            // â”€â”€ Structured log: dead-letter â”€â”€
            const run2 = await AutomationRun.findById(runId);
            if (run2) {
                await logExecution({
                    run: run2,
                    automation: automationId,
                    lead: leadId,
                    nodeId,
                    nodeData,
                    status: 'dead-letter',
                    message: `Exhausted ${job.attemptsMade} retries â€” moved to DLQ`,
                    error: err.message,
                    attempt: job.attemptsMade,
                });
            }
        } catch (e) {
            logger.error(`Error in failed job handler: ${e.message}`);
        }
    } else {
        logger.warn(`âš ï¸ Node "${nodeLabel}" failed (attempt ${job.attemptsMade}/${job.opts?.attempts || 3}): ${err.message} â€” retrying`);

        // Log the retry
        try {
            const run = await AutomationRun.findById(runId);
            if (run) {
                await logExecution({
                    run,
                    automation: automationId,
                    lead: leadId,
                    nodeId,
                    nodeData,
                    status: 'retrying',
                    message: `Attempt ${job.attemptsMade} failed, retrying`,
                    error: err.message,
                    attempt: job.attemptsMade,
                });
            }
        } catch (_) {
            // Never crash the handler over logging
        }
    }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// START / STOP WORKERS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function startWorkers() {
    const connection = getRedisConfig();

    // â”€â”€ Trigger Worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _triggerWorker = new Worker(
        QUEUE_NAMES.TRIGGER,
        processTriggerJob,
        {
            connection,
            concurrency: TRIGGER_CONCURRENCY,
            limiter: { max: 20, duration: 1000 }, // max 20 triggers/sec
        }
    );

    _triggerWorker.on('completed', (job, result) => {
        logger.info(`âœ… Trigger job completed: ${job.name} (matched ${result?.matched || 0})`);
    });

    _triggerWorker.on('failed', (job, err) => {
        logger.error(`âŒ Trigger job failed: ${job?.name} â€” ${err.message}`);
    });

    _triggerWorker.on('error', (err) => {
        // Silently handle worker-level errors (Redis reconnection etc.)
    });

    // â”€â”€ Execute Worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _executeWorker = new Worker(
        QUEUE_NAMES.EXECUTE,
        processExecuteJob,
        {
            connection,
            concurrency: EXECUTE_CONCURRENCY,
        }
    );

    _executeWorker.on('completed', (job, result) => {
        const label = job.data?.nodeData?.label || job.data?.nodeId;
        if (result?.delayed) {
            logger.info(`â±ï¸  Execute job scheduled delay: "${label}" (${result.delayMs}ms)`);
        }
    });

    _executeWorker.on('failed', (job, err) => {
        handleFailedExecuteJob(job, err);
    });

    _executeWorker.on('error', (err) => {
        // Silently handle worker-level errors (Redis reconnection etc.)
    });

    // â”€â”€ Timeout Worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _timeoutWorker = new Worker(
        QUEUE_NAMES.TIMEOUT,
        processTimeoutJob,
        {
            connection,
            concurrency: TIMEOUT_CONCURRENCY,
        }
    );

    _timeoutWorker.on('completed', (job, result) => {
        logger.info(`â° Timeout job completed: ${job.name}`);
    });

    _timeoutWorker.on('failed', (job, err) => {
        logger.error(`âŒ Timeout job failed: ${job?.name} â€” ${err.message}`);
    });

    _timeoutWorker.on('error', (err) => {
        // Silently handle worker-level errors (Redis reconnection etc.)
    });

    logger.info(`ğŸ­ BullMQ Workers started â€” trigger(${TRIGGER_CONCURRENCY}) execute(${EXECUTE_CONCURRENCY}) timeout(${TIMEOUT_CONCURRENCY})`);
}

async function stopWorkers() {
    const promises = [];
    if (_triggerWorker) promises.push(_triggerWorker.close());
    if (_executeWorker) promises.push(_executeWorker.close());
    if (_timeoutWorker) promises.push(_timeoutWorker.close());
    await Promise.allSettled(promises);
    _triggerWorker = null;
    _executeWorker = null;
    _timeoutWorker = null;
    logger.info('BullMQ Workers stopped');
}

function getWorkerStatus() {
    return {
        trigger: _triggerWorker ? 'running' : 'stopped',
        execute: _executeWorker ? 'running' : 'stopped',
        timeout: _timeoutWorker ? 'running' : 'stopped',
    };
}

// â”€â”€â”€ Exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

module.exports = {
    startWorkers,
    stopWorkers,
    getWorkerStatus,
    // Expose for resume/webhook handlers that need to schedule nodes via BullMQ
    scheduleNextNodes,
    updateExecutionPathStatus,
};
